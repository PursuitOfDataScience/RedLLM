# Dream of the Red Chamber (红楼梦) LLM Pretraining

This project is dedicated to pretraining a large language model (LLM) on the classic Chinese novel *Dream of the Red Chamber* (红楼梦). This classic 18th-century novel, written by Cao Xueqin, is my favoriate Chinese classical literature and offers a rich tapestry of character studies, intricate relationships, and profound insights into Qing Dynasty society, making it an ideal corpus for training a language model with a deep understanding of classical Chinese literature and cultural nuance. By leveraging this timeless masterpiece, the goal is to capture its rich narrative style, intricate language, and cultural nuances in a transformer-based model. 

## Overview for 1st pretraining

- **Dataset**: Chapters 1-80 from *Dream of the Red Chamber* (红楼梦) [download link](https://archive.org/details/20210205_20210205_1123/page/16/mode/2up?utm_source=chatgpt.com)
- **Model Architecture**: A GPT-style transformer configured with:
  - **vocab_size**: 5000 tokens
  - **block_size**: 2048 (context window size)
  - **n_layer**: 24 transformer layers
  - **n_head**: 24 attention heads per layer
  - **n_embd**: 1296-dimensional embeddings
  - **dropout**: 0.1
  - **mixed_precision_training**: True
  - **training_type**: Model Parallelism on each node + DDP across nodes
- **Total Parameters**: 499.75 million

## Features

- **Pretraining Objective**: The model is trained in a self-supervised manner to predict the next token, learning the unique style and language of the text.
- **Tokenizer**: Uses a custom tokenizer (with `vocab.json` and `merges.txt`) tailored for classical Chinese.
- **Inference**: Once pretrained, the model can generate text in the style of *Dream of the Red Chamber*, providing insights into classical literature with modern language modeling techniques.

## Compute
32 Intel datacenter GPU nodes (each node equipped with 6 Intel Data Center GPU Max Series with 64G of HBM) to run `pretrain_mp.py`. The entire script took 5.5 hours to finish after runnig for 500 epochs with batch size 24 and learning rate 3e-5.


## Sample Generated Text

Below is an example of text generated by our pre-trained LLM using the "Dream in Red Chamber" dataset:

<pre>

</pre>